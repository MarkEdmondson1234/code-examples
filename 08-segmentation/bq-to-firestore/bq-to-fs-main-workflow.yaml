# https://medium.com/google-cloud/parallel-executions-with-google-workflows-3a16f8fee0eb
# iam roles: 
#  - bq job user
#  - bq data owner
#  - cloud datastore user
#  - logging admin
#  - workflows invoker
# workflows-bq-firestore@learning-ga4.iam.gserviceaccount.com
main:
    params: [args]
    steps:
    - init:
        assign:
            - pageToken: null
            - default_query: >
                  SELECT A.name, A.crm_id, created, job,
                      sum(A.transactions) as sum_crm_trans, 
                      sum(A.revenue) as sum_crm_rev, 
                      sum(sum_web_trans) as sum_web_trans,
                      sum(sum_web_rev) as sum_web_rev,
                  FROM (
                      (SELECT * FROM `learning-ga4.crm_imports.fake_crm_transactions`) AS A
                  LEFT JOIN  
                      (SELECT user_pseudo_id,  
                              count(distinct ecommerce.transaction_id) as sum_web_trans,
                              sum(ecommerce.purchase_revenue_in_usd) as sum_web_rev,
                      FROM `learning-ga4.ga4_public_dataset.events_*`
                      GROUP BY 1) as B
                  ON B.user_pseudo_id = A.cid)
                  GROUP BY 1,2,3,4
                  LIMIT 2000  # for testing, in production remove LIMIT
            - bq_query: ${default(map.get(args, "bq_query"), default_query)}
            - projectId: ${default(map.get(args, "projectId"), sys.get_env("GOOGLE_CLOUD_PROJECT_ID"))}
            - collection: ${default(map.get(args, "collection"), "crm-import")}
            - fs_key: ${default(map.get(args, "fs_key"), "crm_id")}  # change to column holding firestore key
            - maximumBytesBilled: ${default(map.get(args, "maximumBytesBilled"), 1000000000)} #10GB
            - bq_page_size: ${default(map.get(args, "bq_page_size"), 50)} # firestore only accepts batches up to 500.
    - log_init:
        call: sys.log
        args:
            data: ${bq_query + projectId + collection + fs_key + string(maximumBytesBilled) + string(bq_page_size)}
    - startQuery:
        call: googleapis.bigquery.v2.jobs.insert
        args:
            projectId: ${projectId}
            body:
                configuration:
                    query:
                        useLegacySql: false
                        maximumBytesBilled: ${maximumBytesBilled}
                        query: ${bq_query}
        result: query
    - getPage:
        call: googleapis.bigquery.v2.jobs.getQueryResults
        args:
            projectId: ${projectId}
            jobId: ${query.jobReference.jobId}
            maxResults: ${bq_page_size}
            pageToken: ${pageToken}
        result: page
    - extract_schema:
        call: extract_schema
        args:
            page: ${page}
        result: schema_names
    - log_schema:
        call: sys.log
        args:
            data: ${schema_names}
    - parallel-executor:
        call: googleapis.workflowexecutions.v1.projects.locations.workflows.executions.run
        args:
            workflow_id: bq-to-firestore
            argument: 
                page: ${page}
                schema_names: ${schema_names}
                projectId: ${projectId}
                collection: ${collection}
                fs_key: ${fs_key}  # change to column holding firestore key
    - checkIfDone:
        switch:
            - condition: ${"pageToken" in page and page.pageToken != ""}
              assign:
                - pageToken: ${page.pageToken}
              next: getPage


extract_schema:
    params: [page]
    steps:
        - init_schema_list:
            assign:
                - schema_list: []
                - i: 0
                - array: ${page.schema.fields}
        - check_condition:
            switch:
                - condition: ${len(array) > i}
                  next: iterate
            next: exit_loop
        - iterate:
            assign:
              - value: ${array[i].name}
              - schema_list: ${list.concat(schema_list, value)}
              - i: ${i+1}
            next: check_condition
        - exit_loop:
            return: ${schema_list}